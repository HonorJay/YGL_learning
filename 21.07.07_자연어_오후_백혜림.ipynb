{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e79bd3b9",
   "metadata": {},
   "source": [
    "# BPE Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87bc1b62",
   "metadata": {},
   "source": [
    "'l o w e s t'\n",
    "\n",
    "1. n - gram\n",
    "- unigram : 'l', 'o', 'w', 'e', 's', 't'\n",
    "- bigram  : 'lo', 'ow', 'we', 'es', 'st'\n",
    "- trigram : 'low', 'owe', 'wes', 'est'\n",
    "\n",
    "'나는 밥을 먹었어 하지만 배가 고파.'\n",
    "- unigram : '나는', '밥을', '먹었어', '하지만', '배가', '고파'\n",
    "- bigram  : '나는 밥을', '밥을 먹었어', '먹었어 하지만', '하지만 배가', '배가 고파'\n",
    "- trigram : '나는 밥을 먹었어', '밥을 먹었어 하지만', '먹었어 하지만 배가', '하지만 배가 고파'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b7d7766",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98d0655a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_merges = 10 # BEP를 몇 회 수행할 것인지 정함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd3c24a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = {'l o w </w>' : 5,\n",
    "              'l o w e r </w>' : 2,\n",
    "              'n e w e s t </w>' : 6,\n",
    "              'w i d e s t </w>' :3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d65c10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가장 빈도수가 높은 유니그램의 쌍을 하나의 유니그램으로 통합하는 과정으로 num_merges회 반복\n",
    "\n",
    "def get_state(dictionary):\n",
    "    # 유니그램의 pair들의 빈도수를 카운트\n",
    "    pairs = collections.defaultdict(int)\n",
    "    for word, freq in dictionary.items():\n",
    "        symbols = word.split()\n",
    "        for i in range(len(symbols)-1):\n",
    "            pairs[symbols[i], symbols[i+1]] += freq\n",
    "    print('현재 pair들의 빈도수 :', dict(pairs))\n",
    "    return pairs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9e27e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_dictionary(pair, v_in):\n",
    "    v_out = {}\n",
    "    bigram = re.escape(' '.join(pair))\n",
    "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "    for word in v_in:\n",
    "        w_out = p.sub(''.join(pair), word)\n",
    "        v_out[w_out] = v_in[word]\n",
    "    return v_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0dc19b7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> step 1\n",
      "현재 pair들의 빈도수 : {('l', 'o'): 7, ('o', 'w'): 7, ('w', '</w>'): 5, ('w', 'e'): 8, ('e', 'r'): 2, ('r', '</w>'): 2, ('n', 'e'): 6, ('e', 'w'): 6, ('e', 's'): 9, ('s', 't'): 9, ('t', '</w>'): 9, ('w', 'i'): 3, ('i', 'd'): 3, ('d', 'e'): 3}\n",
      "New merge: ('e', 's')\n",
      "dictionary: {'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w es t </w>': 6, 'w i d es t </w>': 3}\n",
      "\n",
      ">> step 2\n",
      "현재 pair들의 빈도수 : {('l', 'o'): 7, ('o', 'w'): 7, ('w', '</w>'): 5, ('w', 'e'): 2, ('e', 'r'): 2, ('r', '</w>'): 2, ('n', 'e'): 6, ('e', 'w'): 6, ('w', 'es'): 6, ('es', 't'): 9, ('t', '</w>'): 9, ('w', 'i'): 3, ('i', 'd'): 3, ('d', 'es'): 3}\n",
      "New merge: ('es', 't')\n",
      "dictionary: {'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w est </w>': 6, 'w i d est </w>': 3}\n",
      "\n",
      ">> step 3\n",
      "현재 pair들의 빈도수 : {('l', 'o'): 7, ('o', 'w'): 7, ('w', '</w>'): 5, ('w', 'e'): 2, ('e', 'r'): 2, ('r', '</w>'): 2, ('n', 'e'): 6, ('e', 'w'): 6, ('w', 'est'): 6, ('est', '</w>'): 9, ('w', 'i'): 3, ('i', 'd'): 3, ('d', 'est'): 3}\n",
      "New merge: ('est', '</w>')\n",
      "dictionary: {'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w est</w>': 6, 'w i d est</w>': 3}\n",
      "\n",
      ">> step 4\n",
      "현재 pair들의 빈도수 : {('l', 'o'): 7, ('o', 'w'): 7, ('w', '</w>'): 5, ('w', 'e'): 2, ('e', 'r'): 2, ('r', '</w>'): 2, ('n', 'e'): 6, ('e', 'w'): 6, ('w', 'est</w>'): 6, ('w', 'i'): 3, ('i', 'd'): 3, ('d', 'est</w>'): 3}\n",
      "New merge: ('l', 'o')\n",
      "dictionary: {'lo w </w>': 5, 'lo w e r </w>': 2, 'n e w est</w>': 6, 'w i d est</w>': 3}\n",
      "\n",
      ">> step 5\n",
      "현재 pair들의 빈도수 : {('lo', 'w'): 7, ('w', '</w>'): 5, ('w', 'e'): 2, ('e', 'r'): 2, ('r', '</w>'): 2, ('n', 'e'): 6, ('e', 'w'): 6, ('w', 'est</w>'): 6, ('w', 'i'): 3, ('i', 'd'): 3, ('d', 'est</w>'): 3}\n",
      "New merge: ('lo', 'w')\n",
      "dictionary: {'low </w>': 5, 'low e r </w>': 2, 'n e w est</w>': 6, 'w i d est</w>': 3}\n",
      "\n",
      ">> step 6\n",
      "현재 pair들의 빈도수 : {('low', '</w>'): 5, ('low', 'e'): 2, ('e', 'r'): 2, ('r', '</w>'): 2, ('n', 'e'): 6, ('e', 'w'): 6, ('w', 'est</w>'): 6, ('w', 'i'): 3, ('i', 'd'): 3, ('d', 'est</w>'): 3}\n",
      "New merge: ('n', 'e')\n",
      "dictionary: {'low </w>': 5, 'low e r </w>': 2, 'ne w est</w>': 6, 'w i d est</w>': 3}\n",
      "\n",
      ">> step 7\n",
      "현재 pair들의 빈도수 : {('low', '</w>'): 5, ('low', 'e'): 2, ('e', 'r'): 2, ('r', '</w>'): 2, ('ne', 'w'): 6, ('w', 'est</w>'): 6, ('w', 'i'): 3, ('i', 'd'): 3, ('d', 'est</w>'): 3}\n",
      "New merge: ('ne', 'w')\n",
      "dictionary: {'low </w>': 5, 'low e r </w>': 2, 'new est</w>': 6, 'w i d est</w>': 3}\n",
      "\n",
      ">> step 8\n",
      "현재 pair들의 빈도수 : {('low', '</w>'): 5, ('low', 'e'): 2, ('e', 'r'): 2, ('r', '</w>'): 2, ('new', 'est</w>'): 6, ('w', 'i'): 3, ('i', 'd'): 3, ('d', 'est</w>'): 3}\n",
      "New merge: ('new', 'est</w>')\n",
      "dictionary: {'low </w>': 5, 'low e r </w>': 2, 'newest</w>': 6, 'w i d est</w>': 3}\n",
      "\n",
      ">> step 9\n",
      "현재 pair들의 빈도수 : {('low', '</w>'): 5, ('low', 'e'): 2, ('e', 'r'): 2, ('r', '</w>'): 2, ('w', 'i'): 3, ('i', 'd'): 3, ('d', 'est</w>'): 3}\n",
      "New merge: ('low', '</w>')\n",
      "dictionary: {'low</w>': 5, 'low e r </w>': 2, 'newest</w>': 6, 'w i d est</w>': 3}\n",
      "\n",
      ">> step 10\n",
      "현재 pair들의 빈도수 : {('low', 'e'): 2, ('e', 'r'): 2, ('r', '</w>'): 2, ('w', 'i'): 3, ('i', 'd'): 3, ('d', 'est</w>'): 3}\n",
      "New merge: ('w', 'i')\n",
      "dictionary: {'low</w>': 5, 'low e r </w>': 2, 'newest</w>': 6, 'wi d est</w>': 3}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bpe_codes = {}\n",
    "bpe_codes_reverse = {}\n",
    "for i in range(num_merges):\n",
    "    print(\">> step {0}\".format(i+1))\n",
    "    pairs = get_state(dictionary)\n",
    "    best = max(pairs, key = pairs.get)\n",
    "    dictionary = merge_dictionary(best, dictionary)\n",
    "    \n",
    "    bpe_codes[best] = i\n",
    "    bpe_codes_reverse[best[0] + best[1]] = best\n",
    "\n",
    "    print(\"New merge: {}\".format(best))\n",
    "    print(\"dictionary: {}\".format(dictionary))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4cf5583b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('e', 's'): 0, ('es', 't'): 1, ('est', '</w>'): 2, ('l', 'o'): 3, ('lo', 'w'): 4, ('n', 'e'): 5, ('ne', 'w'): 6, ('new', 'est</w>'): 7, ('low', '</w>'): 8, ('w', 'i'): 9}\n"
     ]
    }
   ],
   "source": [
    "print(bpe_codes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40dad8ef",
   "metadata": {},
   "source": [
    "## OOV에 대처하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "842cee0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pairs(word):\n",
    "    pairs = set()\n",
    "    prev_char = word[0]\n",
    "    for char in word[1:]:\n",
    "        pairs.add((prev_char, char))\n",
    "        prev_char = char\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "183433e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(orig):\n",
    "    word = tuple(orig) + ('</w>',)\n",
    "    print(\"__word split into characters:__ <tt>{}<tt>\".format(word))\n",
    "    \n",
    "    pairs = get_pairs(word)\n",
    "    \n",
    "    if not pairs:\n",
    "        return orig\n",
    "    \n",
    "    iteration = 0\n",
    "    \n",
    "    while True:\n",
    "        iteration += 1\n",
    "        print(\"__Iteration {}:__\".format(iteration))\n",
    "        \n",
    "        print(\"Bigram in the word: {}\".format(pairs))\n",
    "        bigram = min(pairs, key = lambda pair: bpe_codes.get(pair, float('inf')))\n",
    "        print(\"candidate for merging: {}\".format(bigram))\n",
    "        if bigram not in bpe_codes:\n",
    "            print(\"__Candidate not in BPE merges, algorithm stops.__\")\n",
    "            break\n",
    "            \n",
    "        first, second = bigram\n",
    "        new_word = []\n",
    "        i = 0\n",
    "        while i < len(word):\n",
    "            try:\n",
    "                j = word.index(first, i)\n",
    "                new_word.extend(word[i:j])\n",
    "                i = j\n",
    "            except:\n",
    "                new_word.extend(word[i:])\n",
    "                break\n",
    "                \n",
    "            if word[i] == first and i < len(word)-1 and word[i+1] == second:\n",
    "                new_word.append(first+second)\n",
    "                i += 2\n",
    "            else:\n",
    "                new_word.append(word[i])\n",
    "                i += 1\n",
    "        new_word = tuple(new_word)\n",
    "        word = new_word\n",
    "        print(\"word after merging : {}\".format(word))\n",
    "        if len(word) == 1:\n",
    "            break\n",
    "        else:\n",
    "            pairs = get_pairs(word)\n",
    "            \n",
    "    # 특별토큰인 </w>는 출력하지 않는다.\n",
    "    \n",
    "    if word[-1] == '</w>':\n",
    "        word = word[:-1]\n",
    "    elif word[-1].endswith('</w>'):\n",
    "        word = word[:-1] + (word[-1].replace('</w>', ''),)\n",
    "    return word\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c596c567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__word split into characters:__ <tt>('l', 'o', 'w', 'e', 's', 't', '</w>')<tt>\n",
      "__Iteration 1:__\n",
      "Bigram in the word: {('o', 'w'), ('w', 'e'), ('t', '</w>'), ('l', 'o'), ('e', 's'), ('s', 't')}\n",
      "candidate for merging: ('e', 's')\n",
      "word after merging : ('l', 'o', 'w', 'es', 't', '</w>')\n",
      "__Iteration 2:__\n",
      "Bigram in the word: {('t', '</w>'), ('l', 'o'), ('es', 't'), ('w', 'es'), ('o', 'w')}\n",
      "candidate for merging: ('es', 't')\n",
      "word after merging : ('l', 'o', 'w', 'est', '</w>')\n",
      "__Iteration 3:__\n",
      "Bigram in the word: {('est', '</w>'), ('w', 'est'), ('o', 'w'), ('l', 'o')}\n",
      "candidate for merging: ('est', '</w>')\n",
      "word after merging : ('l', 'o', 'w', 'est</w>')\n",
      "__Iteration 4:__\n",
      "Bigram in the word: {('w', 'est</w>'), ('o', 'w'), ('l', 'o')}\n",
      "candidate for merging: ('l', 'o')\n",
      "word after merging : ('lo', 'w', 'est</w>')\n",
      "__Iteration 5:__\n",
      "Bigram in the word: {('lo', 'w'), ('w', 'est</w>')}\n",
      "candidate for merging: ('lo', 'w')\n",
      "word after merging : ('low', 'est</w>')\n",
      "__Iteration 6:__\n",
      "Bigram in the word: {('low', 'est</w>')}\n",
      "candidate for merging: ('low', 'est</w>')\n",
      "__Candidate not in BPE merges, algorithm stops.__\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('low', 'est')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode('lowest')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d666a34b",
   "metadata": {},
   "source": [
    "## IMDB 리뷰 토큰화하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fd8ee932",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow-datasets in c:\\users\\tuso7\\anaconda3\\envs\\tfgpu\\lib\\site-packages (4.3.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\tuso7\\anaconda3\\envs\\tfgpu\\lib\\site-packages (from tensorflow-datasets) (1.19.5)\n",
      "Requirement already satisfied: protobuf>=3.12.2 in c:\\users\\tuso7\\anaconda3\\envs\\tfgpu\\lib\\site-packages (from tensorflow-datasets) (3.13.0)\n",
      "Requirement already satisfied: six in c:\\users\\tuso7\\anaconda3\\envs\\tfgpu\\lib\\site-packages (from tensorflow-datasets) (1.15.0)\n",
      "Requirement already satisfied: tensorflow-metadata in c:\\users\\tuso7\\anaconda3\\envs\\tfgpu\\lib\\site-packages (from tensorflow-datasets) (1.1.0)\n",
      "Requirement already satisfied: attrs>=18.1.0 in c:\\users\\tuso7\\anaconda3\\envs\\tfgpu\\lib\\site-packages (from tensorflow-datasets) (20.2.0)\n",
      "Requirement already satisfied: promise in c:\\users\\tuso7\\anaconda3\\envs\\tfgpu\\lib\\site-packages (from tensorflow-datasets) (2.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\tuso7\\anaconda3\\envs\\tfgpu\\lib\\site-packages (from tensorflow-datasets) (2.24.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\tuso7\\anaconda3\\envs\\tfgpu\\lib\\site-packages (from tensorflow-datasets) (3.7.4.3)\n",
      "Requirement already satisfied: future in c:\\users\\tuso7\\anaconda3\\envs\\tfgpu\\lib\\site-packages (from tensorflow-datasets) (0.18.2)\n",
      "Requirement already satisfied: absl-py in c:\\users\\tuso7\\anaconda3\\envs\\tfgpu\\lib\\site-packages (from tensorflow-datasets) (0.10.0)\n",
      "Requirement already satisfied: termcolor in c:\\users\\tuso7\\anaconda3\\envs\\tfgpu\\lib\\site-packages (from tensorflow-datasets) (1.1.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\tuso7\\anaconda3\\envs\\tfgpu\\lib\\site-packages (from tensorflow-datasets) (4.59.0)\n",
      "Requirement already satisfied: importlib-resources in c:\\users\\tuso7\\anaconda3\\envs\\tfgpu\\lib\\site-packages (from tensorflow-datasets) (5.2.0)\n",
      "Requirement already satisfied: dill in c:\\users\\tuso7\\anaconda3\\envs\\tfgpu\\lib\\site-packages (from tensorflow-datasets) (0.3.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\tuso7\\anaconda3\\envs\\tfgpu\\lib\\site-packages (from protobuf>=3.12.2->tensorflow-datasets) (52.0.0.post20210125)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\tuso7\\anaconda3\\envs\\tfgpu\\lib\\site-packages (from requests>=2.19.0->tensorflow-datasets) (1.25.11)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\tuso7\\anaconda3\\envs\\tfgpu\\lib\\site-packages (from requests>=2.19.0->tensorflow-datasets) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\tuso7\\anaconda3\\envs\\tfgpu\\lib\\site-packages (from requests>=2.19.0->tensorflow-datasets) (2021.5.30)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\tuso7\\anaconda3\\envs\\tfgpu\\lib\\site-packages (from requests>=2.19.0->tensorflow-datasets) (3.0.4)\n",
      "Requirement already satisfied: zipp>=3.1.0 in c:\\users\\tuso7\\anaconda3\\envs\\tfgpu\\lib\\site-packages (from importlib-resources->tensorflow-datasets) (3.3.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in c:\\users\\tuso7\\anaconda3\\envs\\tfgpu\\lib\\site-packages (from tensorflow-metadata->tensorflow-datasets) (1.53.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow-datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cfbf22e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ec36102a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "404a6007",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('IMDb_Reviews.csv', <http.client.HTTPMessage at 0x2349ddace08>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/LawrenceDuan/IMDb-Review-Analysis/master/IMDb_Reviews.csv\", filename=\"IMDb_Reviews.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "86f17c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('IMDb_Reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b0e4416e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  review  sentiment\n",
      "0      My family and I normally do not watch local mo...          1\n",
      "1      Believe it or not, this was at one time the wo...          0\n",
      "2      After some internet surfing, I found the \"Home...          0\n",
      "3      One of the most unheralded great works of anim...          1\n",
      "4      It was the Sixties, and anyone with long hair ...          0\n",
      "...                                                  ...        ...\n",
      "49995  the people who came up with this are SICK AND ...          0\n",
      "49996  The script is so so laughable... this in turn,...          0\n",
      "49997  \"So there's this bride, you see, and she gets ...          0\n",
      "49998  Your mind will not be satisfied by this nobud...          0\n",
      "49999  The chaser's war on everything is a weekly sho...          1\n",
      "\n",
      "[50000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "61a6affc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        My family and I normally do not watch local mo...\n",
       "1        Believe it or not, this was at one time the wo...\n",
       "2        After some internet surfing, I found the \"Home...\n",
       "3        One of the most unheralded great works of anim...\n",
       "4        It was the Sixties, and anyone with long hair ...\n",
       "                               ...                        \n",
       "49995    the people who came up with this are SICK AND ...\n",
       "49996    The script is so so laughable... this in turn,...\n",
       "49997    \"So there's this bride, you see, and she gets ...\n",
       "49998    Your mind will not be satisfied by this nobud...\n",
       "49999    The chaser's war on everything is a weekly sho...\n",
       "Name: review, Length: 50000, dtype: object"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5be07c06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        1\n",
       "1        0\n",
       "2        0\n",
       "3        1\n",
       "4        0\n",
       "        ..\n",
       "49995    0\n",
       "49996    0\n",
       "49997    0\n",
       "49998    0\n",
       "49999    1\n",
       "Name: sentiment, Length: 50000, dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "64890720",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(train_df['review'], target_vocab_size=2**13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "89adeef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the_', ', ', '. ', 'a_', 'and_', 'of_', 'to_', 's_', 'is_', 'br', 'in_', 'I_', 'that_', 'this_', 'it_', ' /><', ' />', 'was_', 'The_', 't_', 'as_', 'with_', 'for_', '.<', 'on_', 'but_', 'movie_', 'are_', ' (', 'have_', 'his_', 'film_', 'not_', 'be_', 'you_', 'ing_', ' \"', 'ed_', 'it', 'd_', 'an_', 'at_', 'by_', 'he_', 'one_', 'who_', 'from_', 'y_', 'or_', 'e_', 'like_', 'all_', '\" ', 'they_', 'so_', 'just_', 'has_', ') ', 'about_', 'her_', 'out_', 'This_', 'some_', 'movie', 'ly_', 'film', 'very_', 'more_', 'It_', 'what_', 'would_', 'when_', 'if_', 'good_', 'up_', 'which_', 'their_', 'only_', 'even_', 'my_', 'really_', 'had_', 'can_', 'no_', 'were_', 'see_', '? ', 'she_', 'than_', '! ', 'there_', 'been_', 'get_', 'into_', 'will_', ' - ', 'much_', 'n_', 'because_', 'ing']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.subwords[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "16aba7a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretty bad PRC cheapie which I rarely bother to watch over again, and it's no wonder -- it's slow and creaky and dull as a butter knife. Mad doctor George Zucco is at it again, turning a dimwitted farmhand in overalls (Glenn Strange) into a wolf-man. Unfortunately, the makeup is virtually non-existent, consisting only of a beard and dimestore fangs for the most part. If it were not for Zucco and Strange's presence, along with the cute Anne Nagel, this would be completely unwatchable. Strange, who would go on to play Frankenstein's monster for Unuiversal in two years, does a Lenny impression from \"Of Mice and Men\", it seems.<br /><br />*1/2 (of Four)\n"
     ]
    }
   ],
   "source": [
    "print(train_df['review'][20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e5276c96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "토큰화된 샘플 질문: [1590, 4162, 132, 7107, 1892, 2983, 578, 76, 12, 4632, 3422, 7, 160, 175, 372, 2, 5, 39, 8051, 8, 84, 2652, 497, 39, 8051, 8, 1374, 5, 3461, 2012, 48, 5, 2263, 21, 4, 2992, 127, 4729, 711, 3, 1391, 8044, 3557, 1277, 8102, 2154, 5681, 9, 42, 15, 372, 2, 3773, 4, 3502, 2308, 467, 4890, 1503, 11, 3347, 1419, 8127, 29, 5539, 98, 6099, 58, 94, 4, 1388, 4230, 8057, 213, 3, 1966, 2, 1, 6700, 8044, 9, 7069, 716, 8057, 6600, 2, 4102, 36, 78, 6, 4, 1865, 40, 5, 3502, 1043, 1645, 8044, 1000, 1813, 23, 1, 105, 1128, 3, 156, 15, 85, 33, 23, 8102, 2154, 5681, 5, 6099, 8051, 8, 7271, 1055, 2, 534, 22, 1, 3046, 5214, 810, 634, 8120, 2, 14, 71, 34, 436, 3311, 5447, 783, 3, 6099, 2, 46, 71, 193, 25, 7, 428, 2274, 2260, 6487, 8051, 8, 2149, 23, 1138, 4117, 6023, 163, 11, 148, 735, 2, 164, 4, 5277, 921, 3395, 1262, 37, 639, 1349, 349, 5, 2460, 328, 15, 5349, 8127, 24, 10, 16, 10, 17, 8054, 8061, 8059, 8062, 29, 6, 6607, 8126, 8053]\n"
     ]
    }
   ],
   "source": [
    "print(\"토큰화된 샘플 질문: {}\".format(tokenizer.encode(train_df['review'][20])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4335e207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정수 인코딩 후의 문장 [137, 8051, 8, 910, 8057, 2169, 36, 7, 103, 13, 14, 32, 18, 79, 681, 8058]\n",
      "기존 문장: It's mind-blowing to me that this film was even made.\n"
     ]
    }
   ],
   "source": [
    "# 리뷰데이터가 아닌 샘플 문장으로 인코딩하고 디코딩해보자!!\n",
    "sample_string = \"It's mind-blowing to me that this film was even made.\"\n",
    "\n",
    "# 인코딩해서 저장\n",
    "tokenized_string = tokenizer.encode(sample_string)\n",
    "print('정수 인코딩 후의 문장 {}'.format(tokenized_string))\n",
    "\n",
    "# 이를 다시 디코딩하자\n",
    "original_string = tokenizer.decode(tokenized_string)\n",
    "print('기존 문장: {}'.format(original_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "37dee8e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합의 크기(Vocab size) : 8268\n"
     ]
    }
   ],
   "source": [
    "print('단어 집합의 크기(Vocab size) :', tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "46fa7be3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137 ----> It\n",
      "8051 ----> '\n",
      "8 ----> s \n",
      "910 ----> mind\n",
      "8057 ----> -\n",
      "2169 ----> blow\n",
      "36 ----> ing \n",
      "7 ----> to \n",
      "103 ----> me \n",
      "13 ----> that \n",
      "14 ----> this \n",
      "32 ----> film \n",
      "18 ----> was \n",
      "79 ----> even \n",
      "681 ----> made\n",
      "8058 ----> .\n"
     ]
    }
   ],
   "source": [
    "for ts in tokenized_string:\n",
    "      print('{} ----> {}'.format(ts, tokenizer.decode([ts])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "12110e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정수 인코딩 후의 문장 [137, 8051, 8, 910, 8057, 2169, 36, 7, 103, 13, 14, 32, 18, 7974, 8132, 8133, 997, 681, 8058]\n",
      "기존 문장: It's mind-blowing to me that this film was evenxyz made.\n"
     ]
    }
   ],
   "source": [
    "sample_string = \"It's mind-blowing to me that this film was evenxyz made.\"\n",
    "\n",
    "# 인코딩해서 저장\n",
    "tokenized_string = tokenizer.encode(sample_string)\n",
    "print('정수 인코딩 후의 문장 {}'.format(tokenized_string))\n",
    "\n",
    "# 이를 다시 디코딩하자\n",
    "original_string = tokenizer.decode(tokenized_string)\n",
    "print('기존 문장: {}'.format(original_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7e0ec044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137 ----> It\n",
      "8051 ----> '\n",
      "8 ----> s \n",
      "910 ----> mind\n",
      "8057 ----> -\n",
      "2169 ----> blow\n",
      "36 ----> ing \n",
      "7 ----> to \n",
      "103 ----> me \n",
      "13 ----> that \n",
      "14 ----> this \n",
      "32 ----> film \n",
      "18 ----> was \n",
      "7974 ----> even\n",
      "8132 ----> x\n",
      "8133 ----> y\n",
      "997 ----> z \n",
      "681 ----> made\n",
      "8058 ----> .\n"
     ]
    }
   ],
   "source": [
    "for ts in tokenized_string:\n",
    "      print('{} ----> {}'.format(ts, tokenizer.decode([ts])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41158941",
   "metadata": {},
   "source": [
    "## IMDB리뷰 sentencePiece로 토큰화\n",
    "참고 : https://keep-steady.tistory.com/7?category=702926"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2315b42d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.96-cp37-cp37m-win_amd64.whl (1.1 MB)\n",
      "Installing collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.1.96\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3a8f24ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b62c90ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        My family and I normally do not watch local mo...\n",
       "1        Believe it or not, this was at one time the wo...\n",
       "2        After some internet surfing, I found the \"Home...\n",
       "3        One of the most unheralded great works of anim...\n",
       "4        It was the Sixties, and anyone with long hair ...\n",
       "                               ...                        \n",
       "49995    the people who came up with this are SICK AND ...\n",
       "49996    The script is so so laughable... this in turn,...\n",
       "49997    \"So there's this bride, you see, and she gets ...\n",
       "49998    Your mind will not be satisfied by this nobud...\n",
       "49999    The chaser's war on everything is a weekly sho...\n",
       "Name: review, Length: 50000, dtype: object"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('IMDb_Reviews.csv')\n",
    "train_df['review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "eaddb286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "리뷰 개수 :  50000\n"
     ]
    }
   ],
   "source": [
    "print('리뷰 개수 : ', len(train_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1b51af1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('imdb_review.txt', 'w', encoding='utf8') as f:\n",
    "    f.write('\\n'.join(train_df['review']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c785cc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "spm.SentencePieceTrainer.Train('--input=imdb_review.txt --model_prefix=imdb --vocab_size=5000 --model_type=bpe --max_sentence_length=9999')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef858bd",
   "metadata": {},
   "source": [
    "- input : 학습시킬 파일\n",
    "- model_prefix : 만들어질 모델 이름\n",
    "- vocab_size : 단어집합크기\n",
    "- model_type : 사용할 모델(unigram(default), bpe, char, word)\n",
    "- pad_id, pad_piece: pad token id, 값\n",
    "- unk_id, unk_piece : unknown token id, 값\n",
    "- bos_id, bos_piece : begin of sentence token id,값\n",
    "- eos_id, eos_piece : end of sequence token id, 값\n",
    "- user_defined_symbols : 사용자 정의 토큰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "317e9fd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;unk&gt;</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>▁t</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>▁a</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>8</td>\n",
       "      <td>-4992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>4</td>\n",
       "      <td>-4993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>7</td>\n",
       "      <td>-4994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>&amp;</td>\n",
       "      <td>-4995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>6</td>\n",
       "      <td>-4996</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0     1\n",
       "0     <unk>     0\n",
       "1       <s>     0\n",
       "2      </s>     0\n",
       "3        ▁t     0\n",
       "4        ▁a    -1\n",
       "...     ...   ...\n",
       "4995      8 -4992\n",
       "4996      4 -4993\n",
       "4997      7 -4994\n",
       "4998      & -4995\n",
       "4999      6 -4996\n",
       "\n",
       "[5000 rows x 2 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_list = pd.read_csv('imdb.vocab', sep='\\t', header=None, quoting=csv.QUOTE_NONE)\n",
    "vocab_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f7c691fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp = spm.SentencePieceProcessor()\n",
    "vocab_file = 'imdb.model'\n",
    "sp.load(vocab_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7d005c4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I didn't at all think of it this way.\n",
      "['▁I', '▁didn', \"'\", 't', '▁at', '▁all', '▁think', '▁of', '▁it', '▁this', '▁way', '.']\n",
      "[41, 623, 4950, 4926, 138, 169, 378, 30, 58, 73, 413, 4945]\n",
      "\n",
      "I have waited a long time for someone to film\n",
      "['▁I', '▁have', '▁wa', 'ited', '▁a', '▁long', '▁time', '▁for', '▁someone', '▁to', '▁film']\n",
      "[41, 141, 1364, 1120, 4, 666, 285, 92, 1078, 33, 91]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lines = [\n",
    "    \"I didn't at all think of it this way.\",\n",
    "    \"I have waited a long time for someone to film\"\n",
    "]\n",
    "\n",
    "for line in lines:\n",
    "    print(line)\n",
    "    print(sp.encode_as_pieces(line)) # 서브워드 시퀀스로 변환\n",
    "    print(sp.encode_as_ids(line)) # 정수 시퀀스로 변환\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5814c54e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.GetPieceSize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ac2ea843",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'▁character'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.IdToPiece(430) # 정수로부터 매핑되는 서브워드 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "95497248",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "430"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.PieceToId('▁character') # 서브워드로부터 매핑되는 정수로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "90a8c314",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I have waited a long time for someone to film'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.DecodeIds([41, 141, 1364, 1120, 4, 666, 285, 92, 1078, 33, 91])\n",
    "# 정수 시퀀스로부터 문장으로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d98d7a4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I have waited a long time for someone to film'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 서브워드 시퀀스로부터 문장으로 변환\n",
    "sp.DecodePieces(['▁I', '▁have', '▁wa', 'ited', '▁a', '▁long', '▁time', '▁for', '▁someone', '▁to', '▁film'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8c68aa47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁I', '▁have', '▁wa', 'ited', '▁a', '▁long', '▁time', '▁for', '▁someone', '▁to', '▁film']\n",
      "[41, 141, 1364, 1120, 4, 666, 285, 92, 1078, 33, 91]\n"
     ]
    }
   ],
   "source": [
    "# encode는 문장으로부터 인자값에 따라서 정수 시퀀스 또는 서브워드 시퀀스로 변환가능\n",
    "print(sp.encode('I have waited a long time for someone to film', out_type=str))\n",
    "print(sp.encode('I have waited a long time for someone to film', out_type=int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703d9583",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfgpu",
   "language": "python",
   "name": "tfgpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
